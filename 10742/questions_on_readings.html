<html>

<head>
    <style>
        table {
            border-collapse: collapse;
        }

        th,
        td {
            border: 1px solid black;
            padding: 8px;
        }
    </style>
    <style>
        .grey-background {
            background-color: #c0c0c0;
        }
    </style>
</head>

<body>

    <a name="lecture2">Lecture 2</a>

    <ol>

        <strong>(Khan Academy video)</strong>
        <li />Give three examples of payors in the U.S. healthcare ecosystem.

        <li />Briefly define a premium, a co-pay, and a deductible. In the first two cases, who pays and who receives
        the payment?

        <li />How do co-pays attempt to address the issue of moral hazard in healthcare? 
        <br /><br />

    </ol>


    <a name="lecture3">Lecture 3</a>
    <ol>
        <strong>(Population-Level Prediction...)</strong>
        <li />List three advantages of a parsimonious model. Name a disadvantage.
        <li />The paper uses only data sourced from a health insurance company (Blue Cross Independence of PA). The
        study did not use data from any hospital or clinic.
        What are some advantages the authors cite for this approach? Can you think of any drawbacks?
        <li />How do the authors assert that a higher-accuracy predictive algorithm would benefit the DPP?
        <li />The paper relies on a L1-regularized logistic regression model for predicting diabetes.
        In the discussion, they mention (without detail) that this approach was as good or better than
        other model forms, e.g. random forests and neural networks. Even assuming all the models had
        equivalent performance, why might you prefer a logistic regression model here, over a more complex 
        model like xgboost or a neural network?
    </ol>


    <a name="lecture4">Lecture 4</a><br />

    no required readings 
    <br /><br />


    <a name="lecture5">Lecture 5</a><br />

    no required readings 
    <br /><br />



    <a name="lecture6">Lecture 6</a><br />

    <ol>
        <strong>(EHR Safari...)</strong>
        <li /> In section 5.2, the authors describe inconsistent timestamps in the MIMIC dataset.
        Give an argument in favor of discarding such rows from your training data when building a classifier. Give an
        argument against doing so.
        <li /> Section 5.4 describes the problem of duplication in notes. The proposed solution---retaining only the latest
        among a collection of notes sharing the same 'storetime' timestamp---eliminates redundant drafts of the same note. 
        However, it doesn't entirely solve the duplicate-text problem, since providers often copy and paste sections of previous notes into a new
        progress note. What kind of biases might this "copy-paste" habit 
        introduce into the clinical record? If you could build a 100% reliable tool to identify text that was copy-pasted from a previous note, should 
        often use that tool to remove copy-pasted text? Why or why not?

        <br /><br />

        <strong>(Dissecting Racial Bias...)</strong>
        <li /> Why does Figure 3b explain the difference in the orange and purple curves in Figure 2?
        <li /> The authors establish that future health costs are an unreliable proxy for future health status. What is
        their main argument for this?
        <li /> You are the VP of Care Management at UniCitna, a large health insurer which uses this algorithm to
        identify members who should receive additional services.
        Recognizing the disparity between Black and White members, you propose to adjust the threshold as follows:

        <ul>
            <li /> OLD POLICY: patients above the 97% threshold are enrolled in the program.

            <li /> NEW POLICY: patients above the 97% threshold are enrolled in the program, and Black patients above a
            93% threshold are also enrolled in the program.
        </ul>
        Give one argument for and one argument against this revised policy.
    </ol>

    <a name="lecture7">Lecture 7</a><br />
    <ol>
        <strong>(Medicare Overview - Khan Academy video)</strong>
        <li/>Who is eligible for Medicare? Medicaid?
        <li/>In no more than 2 sentences each, describe each of these programs: Medicare Parts A,B,C,D.  
    </ol><br /><br />

    <a name="lecture8">Lecture 8</a><br />
    (waiting to see if guest lecturer wants to change the readings)<br /><br />

    <a name="lecture9">Lecture 9</a>
    <ol>
        <strong>(Chexpert)</strong>
        <li /> The paper concedes that one limitation of the study is that neither the algorithm nor the expert
        radiologists (serving as a baseline) had access to the patient's clinical history,
        nor any previous exams. Think of a specific clinical scenario where this might be a serious limitation for a
        radiologist. Briefly describe how you might design an enhanced model architecture that could take
        into account previously documented clinical findings.
        <li />The authors spend some time describing how they assign one or more labels (e.g. "cardiomegaly" and
        "pleural effusion") to each image. They describe the shortcomings of this approach.
        If you had infinite time and a panel of expert radiologists, how would you approach the labeling problem?
    </ol>

    <a name="lecture10">Lecture 10</a><br />
    <ol>
    <strong>Revolutionizing Digital Pathology</strong>
   <li/>Section 4.4 refers to pathologists as the "doctor's doctor." Why is this? 
   <li/>Briefly define and distinguish these terms: pathology, histology, histopathology.
   <li/>The term 'zebra' in section 4.3 is medical slang. It alludes to the common 
   medical expression "When you hear hoofbeats, think horses, not zebras," which is often taught to medical students. Restate this in Bayesian terms. 
   <li/>The paper mentions (page 29) that foundation models could help in the standardized extraction of information from pathology reports. 
   List five common structured fields that one might wish to extract from a (text) pathology report. 
   <br/>
   <strong>Learning to predict RNA sequence expressions...</strong>
   <li/>What is multiple instance learning and why do the authors need to rely on it in this setting? 
   <li/>The paper describes a multi-task learner.  Referring to Fig 1, what do the two heads predict? 
    </ol>
    none<br /><br />


    <a name="lecture11">Lecture 11</a><br />

    <ol>
       
       <strong>Artificial intelligence and deep learning in ophthalmology...</strong>
       <li/>In the "Potential Challenges section," the authors write the following. In your own
       words, at a high level and using no more than 3-4 sentences, what are the authors concerned about?
       <li/>

<blockquote>
       Second, many AI groups have reported robust diagnostic
       performance for their DL systems, although some papers did not
       show how the power calculation was performed for the independent data sets. 
       A power calculation should take the following into
       consideration: the prevalence of the disease, type 1 and 2 errors,
       CIs, desired precision and so on. It is important to first preset
       the desired operating threshold on the training set, followed by
       analysis of performance metrics such as sensitivity and specificity
       on the test set to assess calibration of the algorithm.

    </blockquote>




       <li/>blah
       <strong>A foundation model for generalizable disease detection from retinal images</strong>
        <li/>The paper explores several different approaches to SSL: one generative (MAE), 
             and four contrastive (among which DINO performed the best). Which approach worked the best? 
             Briefly describe how the authors implement that approach. Do you have any intuition for why 
             that approach worked best in this application?
         <li/>The "RETFound architecture and implementation" section at the end of the article provides 
             details on the MAE implementation. What would be the likely effect of increasing or decreasing the
             mask ratio?  
             <li/>The ethics statement on page 165 starts with <it>This study involves human participants and was approved by the
                London-Central Research Ethics Committee</it>, which is the rough equivalent in the UK of an IRB review. 
                Why do you think the authors felt obliged to get an IRB review of this work?
    </ol>
    
    <br /><br />

    <a name="lecture12">Lecture 12</a><br />
    <ol>
        <strong>(Paving the COWPath...)</strong>
        <li />The paper does not report on evaluating the quality of these derived clinical pathways. How might you
        design an experiment to measure (numerically) the quality of the derived pathways?
        <li />The paper describes "supernodes" which are tuples of (visit purpose, procedure, medication, diagnosis).
        There are 3276 unique supernode values. There will naturally be a lot of sparsity in
        this data -- i.e. some supernodes which rarely or never occur in the data. How might you extend this approach to
        address this issue?
        <li />How might you use the learned pathways in clinical practice? <br /><br />

        <strong>(The Artificial Intelligence Clinician Learns...)</strong>
        <li />Reinforcement learning(RL) differs from other kinds of ML; in the RL scenario, you typically face not just
        one classification or regression problem, but
        instead a sequence of many decisions over time. The paper describes applying RL to managing sepsis. Can you
        think of two other problems in healthcare where
        RL might be applicable?
        <li />What is off-policy learning, and why did the authors rely on it in this study?
        <li />With RL, defining the reward function R has a huge impact on the learned policy. The paper defines the
        reward function very simply as follows: +100 if the patient
        survived, and -100 if the patient died. This is an example of a "sparse reward" function. What problems might
        this introduce? Can you think of an intermediate reward function
        that might help?
        <li />The authors mention the following in the 'Methods' section. What concerns might you have with this?
        <em>
            We estimated the transition matrix T(s′,s,a) by counting how many times
            each transition was observed in the MIMIC-III training dataset and converting
            the transition counts to a stochastic matrix32. In high-risk environments (where
            executing a bad policy could cause harm) limiting the action space to known
            options is a sensible choice to increase the safety of the model. We restricted the
            set of actions to choose from to frequently observed actions taken by clinicians
            and excluded transitions seen fewer than five times. As such, the resulting AI
            policy suggests the best possible treatment among all the options chosen (relatively
            frequently) by clinicians.
        </em>
        <br/><br/>

        <strong>(Does the Artificial Intelligence Clinician...)</strong> 

        <li/>Explain briefly how importance sampling works in the "AI Clinician" paper. 
        <li/> Explain briefly how importance sampling may introduce bias in the reward function, and thus 
        may cause the AI Clinician to learn the wrong policy.  
        <li/> Explain briefly how the imbalance in the dataset (nearly 2/3 of the patients are healthy)
        can lead to the AI clinician learning a "zero drug" policy.


    </ol>

    <a name="lecture4">Lecture 13</a><br />
    <ol>
        <strong>The Artificial Intelligence Clinician Learns...</strong>
        <li/>The paper describes an RL framework with a sparse reward signal. What are some drawbacks of this approach? Can you think of a viable, "less spiky" reward function to use instead? 
        <li/>Describe how the paper employes deep learning. 
    </ol><br /><br />

    <a name="lecture5">Lecture 14</a><br />
    none<br /><br />

    <a name="lecture4">Lecture 15</a><br />
    <ol>
        <strong>DeepPatient...</strong>
        <li/>The section "EHR Processing" describes how the authors process the clinical notes using several techniques, e.g. negation detection and topic modeling using LDA. 
        Recognizing that this paper was published in the "pre-transformer" era (2016), how might you use 'modern' techniques to 
        process clinical notes into a real-valued vector?
        <li/>Reading the "Dataset" section, do you believe the authors did an adequate job preventing <a href="https://towardsdatascience.com/other-ml-jargons-label-leakage-9e85b22c6fd0">label leakage</a>? 
        If yes, explain the authors' approach and why it is adequate. If not, explain how you could reduce this risk.
        <li/>This work does not account for the temporal aspect of a patient's record. For example, a series of prescriptions or procedures over time might increase the 
        likelihood of a certain future diagnosis. How might you extend this approach to account for how items occur over time in the patient record?<br/><br/>
        
        <strong>(Clinical Concept Embeddings...)</strong>
        <li/>How might you use the cui2vec embeddings to discover <a href="https://www.cancer.gov/about-cancer/treatment/drugs/off-label">off-label drug use</a>?

        <li/>Imagine that <i></i>n September of 2019, Kevin decided to use this paper's approach to build a medical word embeddings vocabulary. 
        Now, in 2024, Kevin's embeddings are out of date. Many concepts (e.g. Covid-19) aren't captured in the data, and 
        recently-discovered medical knowledge (e.g. On Dec 8 2023, the FDA <a href="https://www.sciencenews.org/article/first-crispr-therapy-sickle-cell-fda">approved</a> a CRISPR-based therapy for sickle-cell disease) is also missing. Given a collection of recently-created
        healthcare-related documents, describe two approaches to enable Kevin to create an updated or "current" version of his embeddings to reflect new domain knowledge.
        <br/><br/>
        <strong>(ClinicalBERT)</strong>
        <li/>The authors describe a system built using the BERT framework and fine-tuned to the task of predicting hospital readmission. Starting from 
         their trained model, how would you build a model to predict in-hospital mortality? (Hint: almost all of the trained parameters from the
         original model should be unaffected.)
         <li/>Explain what's going on in Table 2
         <li/>Can ClinicalBERT be easily repurposed in a generative application, like summarizing an EHR patient chart or chatting with a patient about their medical condition? Why or why not?
   </ol>
    
    
    <br /><br />

    <a name="lecture5">Lecture 16</a><br />
    none<br /><br />

    <a name="lecture4">Lecture 17</a><br />
    <ol>
    <strong>PRISM: Patient Records Interpretation for Semantic Clinical Trial Matching...</strong>
    <li/>Figure 4a shows that when PRISM (using OncoLM) ranked trials for a given patient, in only ~60% of the time did the "ground truth trial" appear in the top 3. 
    There are two obvious explanations for this, one unfavorable to PRISM and one favorable. What are they? Does paragraph under the heading  "Trial Level" (in Section 3.3) 
    favor one or the other explanation? 
    <li/> Referring to figure 1 in the supplementary material. Per the paper, the average agreement between medical doctors ("inter-annotator agreement") --calculated as the percentage of times the 
    annotators gave same answer to a particular question among all five annotators -- was 64%. 
    That's a pretty low number, and the annotators were only performing a rather narrow task: classify a chunk as answering 'yes' or 'no' to a given question. How might you 
    address this issue? 
    <strong>Zero-Shot Clinical Trial Patient Matching with LLMs</strong>
    <li/>Section 3.3 describes the retrieval pipeline. What are some considerations in selecting a value for k?
    <li/>UPMC Hillman Cancer Center sees about 140,000 patients every year. There are around 400,000 active trials at clinicaltrials.gov. Using back of the envelope math, 
    sketch the rough costs of running this zero-shot clinical trial matching approach on all (patient, trial) pairs. Does the proposed pre-filtering approach in the paper 
    fully address this problem? If not, what additional measure(s) would you propose, and where in the retrieval pipeline (M, Phi, or A) would it live? 
    </ol>
    none<br /><br />

    <a name="lecture5">Lecture 18</a><br />
    none<br /><br />

    <a name="lecture4">Lecture 19</a><br />
    none<br /><br />

    <a name="lecture5">Lecture 20</a><br />
    none<br /><br />

    <a name="lecture4">Lecture 21</a><br />
    <ol>
        <strong>Bayesian clinical trials</strong>
        <li/> In Figure 2, if we replace the non-informative prior with a highly opinionated prior where p(0)=1, how would the rest of the graphs look? 
        (Note: this case is sometimes called the "tyranny of the prior")
        <li/> On page 33, in the "What prior" box is this phrase: "it is not possible to find the probability of having a disease based on test results without
        specifying the disease’s prevalence." Write this in terms of Bayes's rule. 
        <strong>The REMAP-CAP...</strong>
        <li/> The paper mentions (in Figure 1) that the REMAP platform can run "perpetually." What do they mean by this?
        <li/>What is Response-Adaptive Randomization (RAR) and how is it implemented in REMAP-CAP?
        <li/>Dr. Aarskog has developed a new gene therapy treatment for a rare disease. He would like to assess the efficacy of this treatment. Should 
        she use REMAP? Why or why not?
        <li/>List some ethical considerations in selecting a frequentist RCT vs. a Bayesian trial. Be sure to include at least one argument in favor and against the Bayesian approach.
    </ol>
    <br /><br />

    <a name="lecture5">Lecture 22</a><br />
    none<br /><br />

    <a name="lecture4">Lecture 23</a><br />
    none<br /><br />

    <a name="lecture5">Lecture 24</a><br />
    none<br /><br />

</body>

</html>